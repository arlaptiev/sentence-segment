{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from mosestokenizer import MosesDetokenizer\n",
    "from nltk.corpus import brown\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Segmenter:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Loads data for training and testing a CRF model'''\n",
    "        data = self.get_sents()\n",
    "\n",
    "        X = [self.sent2features(sent) for sent in data]\n",
    "        y = [self.sent2labels(sent) for sent in data]\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    def label_sents(self, sents):\n",
    "        '''Returns labeled sents: 'S' corresponding to a token right before a Space, and 'P' corresponding\n",
    "        to a token right before a period'''\n",
    "        #\n",
    "        labeled_sents = []\n",
    "        for s in sents:\n",
    "            pairs = []\n",
    "            # Label every token 'S' if a space goes after it or 'P' if a period follows\n",
    "            for i in range(len(s) - 2):\n",
    "                pairs.append([s[i], 'S'])\n",
    "\n",
    "            pairs.append([s[len(s) - 2], 'P'])\n",
    "\n",
    "            labeled_sents.append(pairs)\n",
    "        return labeled_sents\n",
    "\n",
    "    def get_sents(self):\n",
    "        '''Returns joined, labeled, and POS tagged sentences from Brown corpus with no punctuation in between'''\n",
    "        # Read data corpus into sents\n",
    "        sents = brown.sents()\n",
    "        labeled_sents = self.label_sents(sents)\n",
    "\n",
    "        # Join every two sentence\n",
    "        paragraphs = [a + b for a, b in zip(labeled_sents[::2], labeled_sents[1::2])]\n",
    "\n",
    "        '''TODO for comma splice\n",
    "        # Join every two sentence\n",
    "        p1 = [a[:-1] + [[a[-1][0]] + ['S']] + [[',', 'P']] + b for a,\n",
    "              b in zip(labeled_sents[::4], labeled_sents[1::4])]\n",
    "        # Join every other two sentence with a comma after the first one - comma splice\n",
    "        p2 = [a + b for a, b in zip(labeled_sents[2::4], labeled_sents[3::4])]\n",
    "\n",
    "        paragraphs = p1 + p2'''\n",
    "\n",
    "        data = []\n",
    "        for p in paragraphs:\n",
    "            # Delete the 'P' label from the end of the second sentence\n",
    "            p[len(p) - 1][1] = 'S'\n",
    "\n",
    "            # Obtain the list of tokens in the document\n",
    "            tokens = [t for t, label in p]\n",
    "\n",
    "            # Perform POS tagging\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "            # Take the word, POS tag, and its label\n",
    "            data.append([(w, pos, label) for (w, label), (word, pos) in zip(p, tagged)])\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def is_perplexity_decreasing(items):\n",
    "    \"\"\" Returns true if per-word perplexity of the sentence decreases if a period is inserted in front of the items \"\"\"\n",
    "    arr = []\n",
    "    for i in (items, items + ['.']):\n",
    "        padded_grams = ngrams(i, n)\n",
    "        arr.append(model.perplexity(padded_grams))\n",
    "    return arr[0] > arr[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def word2features(sent, i):\n",
    "        word = sent[i][0]\n",
    "        postag = sent[i][1]\n",
    "        label_seq = sent2labels(sent[:i + 1])\n",
    "        postag_seq = sent2postags(sent[:i + 1])\n",
    "\n",
    "        # Common features for all words\n",
    "        features = [\n",
    "            'bias',\n",
    "            'word.lower=' + word.lower(),\n",
    "            'word[-3:]=' + word[-3:],\n",
    "            'word[-2:]=' + word[-2:],\n",
    "            'word.isupper=%s' % word.isupper(),\n",
    "            'word.isdigit=%s' % word.isdigit(),\n",
    "            'postag=' + postag,\n",
    "            'wordperpl.isdecr=%s' % is_perplexity_decreasing(label_seq),\n",
    "            'postagperpl.isdecr=%s' % is_perplexity_decreasing(postag_seq)\n",
    "        ]\n",
    "\n",
    "        # Features for words that are not\n",
    "        # at the beginning of a document\n",
    "        if i > 0:\n",
    "            word1 = sent[i-1][0]\n",
    "            postag1 = sent[i-1][1]\n",
    "            features.extend([\n",
    "                '-1:word.lower=' + word1.lower(),\n",
    "                '-1:word.isupper=%s' % word1.isupper(),\n",
    "                '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '-1:postag=' + postag1\n",
    "            ])\n",
    "        else:\n",
    "            # Indicate that it is the 'beginning of a document'\n",
    "            features.append('BOS')\n",
    "\n",
    "        # Features for words that are not\n",
    "        # at the end of a document\n",
    "        if i < len(sent)-1:\n",
    "            word1 = sent[i+1][0]\n",
    "            postag1 = sent[i+1][1]\n",
    "            features.extend([\n",
    "                '+1:word.lower=' + word1.lower(),\n",
    "                '+1:word.isupper=%s' % word1.isupper(),\n",
    "                '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '+1:postag=' + postag1\n",
    "            ])\n",
    "        else:\n",
    "            # Indicate that it is the 'end of a document'\n",
    "            features.append('EOS')\n",
    "\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def sent2features(sent):\n",
    "        '''A function for extracting features from sentences'''\n",
    "        return [Segmenter.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "    @staticmethod\n",
    "    def sent2labels(sent):\n",
    "        '''A function for generating the output list of labels for each sentence'''\n",
    "        return [label for (token, postag, label) in sent]\n",
    "\n",
    "    @staticmethod\n",
    "    def sent2postags(sent):\n",
    "        '''A function for generating the output list of postags for each sentence'''\n",
    "        return [postag for (token, postag) in sent]\n",
    "\n",
    "    @staticmethod\n",
    "    def sent2tokens(sent):\n",
    "        '''A function for generating the output list of postags for each sentence'''\n",
    "        return [token for (token, postag) in sent]\n",
    "\n",
    "    def train(self):\n",
    "        crf = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "\n",
    "            # coefficient for L1 penalty\n",
    "            c1=0.8377072127476861,\n",
    "\n",
    "            # coefficient for L2 penalty\n",
    "            c2=4.083015357819278e-05,\n",
    "\n",
    "            # maximum number of iterations\n",
    "            max_iterations=200,\n",
    "\n",
    "            # whether to include transitions that\n",
    "            # are possible, but not observed\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        # Submit training data to the trainer\n",
    "        crf.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Save the model into 'crf.model' file\n",
    "        self.save_model(crf)\n",
    "\n",
    "    def test(self):\n",
    "        # Load model\n",
    "        crf = self.load_model()\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = crf.predict(self.X_test)\n",
    "\n",
    "        # Random sample in the testing set\n",
    "        i = 6\n",
    "        for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in self.X_test[i]]):\n",
    "            print(\"%s (%s)\" % (y, x))\n",
    "\n",
    "        # Create a mapping of labels to indices\n",
    "        labels = list(crf.classes_)\n",
    "\n",
    "        # Print out the classification report\n",
    "        print(metrics.flat_classification_report(\n",
    "            self.y_test, y_pred, labels=labels, digits=3\n",
    "        ))\n",
    "\n",
    "    def save_model(self, crf):\n",
    "        with open('crf.model', 'wb') as f:\n",
    "            pickle.dump(crf, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(name):\n",
    "        with open(name, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(tokens):\n",
    "        '''Returns predicted labels of the string'''\n",
    "        X = Segmenter.sent2features(nltk.pos_tag(tokens))\n",
    "        return Segmenter.load_model().predict([X])[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def segment_sent(s):\n",
    "        '''Segments a sentence pased on prediction'''\n",
    "        tokens = word_tokenize(s)\n",
    "        y = Segmenter.predict(tokens)\n",
    "\n",
    "        sents = []\n",
    "        with MosesDetokenizer('en') as detokenize:\n",
    "            if 'P' in y:\n",
    "                n = y.index('P') + 1\n",
    "                sents.append(detokenize(tokens[:n]))\n",
    "                sents.append(detokenize(tokens[n:]))\n",
    "            else:\n",
    "                sents.append(s)\n",
    "\n",
    "        return sents\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # s = Segmenter()\n",
    "    # s.train()\n",
    "    # s.test()\n",
    "    print(Segmenter.segment_sent('Get data as a string from readin and write out the segments'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosestokenizer import MosesDetokenizer\n",
    "from nltk.corpus import brown\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-05518887eb3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent2features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-05518887eb3e>\u001b[0m in \u001b[0;36msent2features\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msent2features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;34m'''A function for extracting features from sentences'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword2features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-05518887eb3e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msent2features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;34m'''A function for extracting features from sentences'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword2features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-05518887eb3e>\u001b[0m in \u001b[0;36mword2features\u001b[1;34m(sent, i)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;34m'word.isdigit=%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;34m'postag='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpostag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;34m'wordperpl.isdecr=%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mis_perplexity_decreasing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'words_language.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m#'postagperpl.isdecr=%s' % is_perplexity_decreasing(postag_seq)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     ]\n",
      "\u001b[1;32m<ipython-input-2-05518887eb3e>\u001b[0m in \u001b[0;36mis_perplexity_decreasing\u001b[1;34m(items, n, modelname)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpadded_grams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_grams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramFiles\\Anaconda\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mperplexity\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ngrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramFiles\\Anaconda\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36mentropy\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m    189\u001b[0m         return -1 * _mean(\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_ngrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramFiles\\Anaconda\\lib\\site-packages\\nltk\\lm\\api.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(items)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;34m\"\"\"Return average (aka mean) for sequence of items.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def load_model(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def is_perplexity_decreasing(items, n, modelname):\n",
    "    \"\"\" Returns true if per-word perplexity of the sentence decreases if a period is inserted in front of the items \"\"\"\n",
    "    arr = []\n",
    "    for i in (items, items + ['.']):\n",
    "        padded_grams = ngrams(i, n)\n",
    "        arr.append(load_model(modelname).perplexity(padded_grams))\n",
    "    return arr[0] > arr[1]\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    label_seq = sent2tokens(sent[:i + 1])\n",
    "    #postag_seq = sent2postags(sent[:i + 1])\n",
    "\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'wordperpl.isdecr=%s' % is_perplexity_decreasing(label_seq, 3, 'words_language.model')\n",
    "        #'postagperpl.isdecr=%s' % is_perplexity_decreasing(postag_seq)\n",
    "    ]\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a document\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    '''A function for extracting features from sentences'''\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    '''A function for generating the output list of labels for each sentence'''\n",
    "    return [label for (token, postag, label) in sent]\n",
    "\n",
    "\n",
    "def sent2postags(sent):\n",
    "    '''A function for generating the output list of postags for each sentence'''\n",
    "    return [postag for (token, postag) in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    '''A function for generating the output list of postags for each sentence'''\n",
    "    return [token for (token, postag) in sent]\n",
    "\n",
    "\n",
    "X = sent2features(nltk.pos_tag(brown.sents()[0]))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    print(sent2tokens(sent[:i + 1]))\n",
    "    \n",
    "sent2features(nltk.pos_tag(brown.sents()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
